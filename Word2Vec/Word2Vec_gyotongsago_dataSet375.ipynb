{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jongil Park\\AppData\\Local\\conda\\conda\\envs\\python_study\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from konlpy.tag import Komoran;\n",
    "k = Komoran()  \n",
    "import nltk  \n",
    "from nltk import FreqDist \n",
    "import pandas as pd\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 테스트 케이스\n",
    "- '교통사고'키워드로 검색한 판례 375개를 Word2Vec 모델의 데이터 셋으로 활용한 테스트 케이스1.\n",
    "\n",
    "두번째 테스트 케이스(2271개 판례 활용)\n",
    "- 5개의 카테고리(교통사고, 교통, 사고, 운전, 차량)를 사용하여 Word2Vec 모델의 데이터 셋으로 활용한 테스트 케이스2.\n",
    "\n",
    "\n",
    "이 코드는 첫번째 테스트 케이스의 내용으로 구성하였습니다.\n",
    "\n",
    "데이터 전처리 작업 및 \n",
    "사용자의 입력값(Ex. 도로에서 음주 후 음주운전 중에 사고가 났고, 당시 알콜 농도는 0.15였습니다)을 토크나이징하여\n",
    "단어값들과 가장 연관성이 높은 Word2Vec모델의 값을 이용할 예정이며, 그 값이 높을수록 해당 판례와 관련성이 많다고 가정하였습니다.\n",
    "때문에 각 판례들을 각각 파싱하여 단어 구성과 단어의 갯수를 딕셔너리로 구성합니다 - \"1차 완료\"부분\n",
    "\n",
    "'교통사고'판례에서 나온 단어들을 토크나이징하고 그 중 사용할 단어와 사용하지 않을 단어 셋을 구성하였고,\n",
    "사용하지 않을 단어를 제외한 Word2Vec모델에 넣을 단어 말뭉치를 구성하였습니다 - \"2차 완료\"부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1차 완료: 각 타이틀, 사건번호, 딕셔너리(단어: 번호)매칭\n"
     ]
    }
   ],
   "source": [
    "# 1차적으로 사용하지 않을 키워드, 판례에서 필요한 부분(판례 제목, 사건 번호 등을 따로 뽑아 놓음)\n",
    "fileName_dnusing_wordSet = 'C:/Users/Jongil Park/PycharmProjects/ai_study/New_projectDirectory_law2' \\\n",
    "                           '/DataSet/Gyotongsago_data_375/Gyotongsago_dataSet/Gyotongsago_dnUsingWordSet_375_onlyNoun.txt'\n",
    "fileName_title = 'C:/Users/Jongil Park/PycharmProjects/ai_study/New_projectDirectory_law2' \\\n",
    "                 '/DataSet/Gyotongsago_data_375/Gyotongsago_dataSet/Gyotongsago_lawTitle_375.txt'\n",
    "fileName_keyNum = 'C:/Users/Jongil Park/PycharmProjects/ai_study/New_projectDirectory_law2' \\\n",
    "                  '/DataSet/Gyotongsago_data_375/Gyotongsago_dataSet/Gyotongsago_lawNumber_375.txt'\n",
    "\n",
    "## 불러온 파일 리스트화\n",
    "with open(fileName_dnusing_wordSet, 'r') as infile:\n",
    "    word_list = [line.rstrip() for line in infile]\n",
    "\n",
    "## 타이틀만 모아놓은 것 리스트로 만들기\n",
    "with open(fileName_title, 'r') as infile2:\n",
    "    title_list = [line.rstrip() for line in infile2]\n",
    "\n",
    "## 키워드 넘버만 모아놓은 것 리스트 만들기\n",
    "try:\n",
    "    with open(fileName_keyNum) as infile3:\n",
    "        keyNum_list = [line.rstrip() for line in infile3]\n",
    "except UnicodeDecodeError:   ## TODO: 텍스트 파일을 불러올 때, 에러가 나는 이유는 무엇인가? 왜 그래서 codecs를 써야 하는가?\n",
    "    with codecs.open(fileName_keyNum, \"r\", \"utf-8\") as infile3:\n",
    "        keyNum_list = [line.rstrip() for line in infile3]\n",
    "\n",
    "\n",
    "## 여기서 pasing 된 corpus 값을 명사만 추출하여 각 단어별 갯수를 딕셔너리로 만들어주고 append\n",
    "def append_noun_words(corpus):\n",
    "    noun_words = ['NNG', 'NNB', 'NP']  # 일반명사, 고유명사, 대명사만 학습  // 이렇게 한 이유에 대해서?\n",
    "    results = []\n",
    "    for text in corpus:\n",
    "        for noun_word in noun_words:\n",
    "            if noun_word in text[1]:\n",
    "                results.append(text[0])\n",
    "    return results\n",
    "\n",
    "## {판례의 명사 키워드의 단어:갯수 매칭된 리스트}\n",
    "total_panrye_parsingReason = []  # 이 안의 값은 딕셔너리가 되어야 함\n",
    "\n",
    "## '교통사고' 하나의 카테고리만 활용할 때,\n",
    "file_name = ['C:/Users/Jongil Park/PycharmProjects/ai_study/New_projectDirectory_law2/DataSet/Gyotongsago_data_375'\n",
    "             '/Gyotongsago_dataSet/Gyotongsago_reason_375/({0}).txt']\n",
    "file_len = [375]\n",
    "\n",
    "for z in range(len(file_name)):\n",
    "\n",
    "    for i in range(file_len[z]):    # len(title_list)\n",
    "        ione_panrye_parsingReason = {}\n",
    "\n",
    "        try:\n",
    "            with open(file_name[z].format(i+1), 'r') as f:\n",
    "                texts = f.read()\n",
    "                corpus = k.pos(\"\\n\".join([s for s in texts.split(\"\\n\") if s]))\n",
    "        except UnicodeDecodeError:\n",
    "            with codecs.open(file_name[z].format(i+1), \"r\", \"utf-8\") as f:\n",
    "                texts = f.read()\n",
    "                corpus = k.pos(\"\\n\".join([s for s in texts.split(\"\\n\") if s]))\n",
    "\n",
    "        corpus = append_noun_words(corpus)\n",
    "        corpus_ko = nltk.Text(corpus, name=\"각 판례별 명사 처리\")\n",
    "        corpus_ko_vocab = corpus_ko.vocab()\n",
    "        corpus_vocab_common = corpus_ko_vocab.most_common(len(corpus_ko_vocab))\n",
    "\n",
    "    ## 여기에 필요한 키워드 값만 추출하는 것을 추가 할 것.\n",
    "        for j in range(len(corpus_vocab_common)):\n",
    "            if corpus_vocab_common[j][0] not in word_list:\n",
    "                ione_panrye_parsingReason[corpus_vocab_common[j][0]] = corpus_vocab_common[j][1]\n",
    "        total_panrye_parsingReason.append(ione_panrye_parsingReason)\n",
    "\n",
    "print(\"1차 완료: 각 타이틀, 사건번호, 딕셔너리(단어: 번호)매칭\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec 전, 2차 완료(Word2Vec모델에 넣을 단어 말뭉치 형성)\n",
      "len(total_sumWord):  63352\n"
     ]
    }
   ],
   "source": [
    "#======================================================================================================\n",
    "## TODO: 3번: W2V을 위한 데이터 전처리, '기준'을 어떻게 둘 것인가?\n",
    "\"\"\"최종적으로 데이터 프레임화를 위한 리스트\"\"\"\n",
    "# TODO: 판례 데이터 사용방법 \n",
    "# 1. '교통사고'의 이유 부분 전체 합을 이용\n",
    "# 2. '교통사고' 각 판례(375개)를 각각 W2V 모델에 사용하고 그 값들을 활용?\n",
    "\n",
    "# 각 교통사고 관련 카테고리의 '이유' 부분의 sum 값을 활용\n",
    "data_file = ['C:/Users/Jongil Park/PycharmProjects/ai_study/New_projectDirectory_law2/DataSet/Gyotongsago_data_375'\n",
    "             '/Gyotongsago_dataSet/Gyotongsago_totalSum_375.txt']\n",
    "\n",
    "total_sumWord = []\n",
    "for i in range(len(data_file)):\n",
    "    try:\n",
    "        with open(data_file[i]) as f:\n",
    "            texts = f.read()\n",
    "            corpus = k.pos(\"\\n\".join([s for s in texts.split(\"\\n\") if s]))  # 이것의 의미 다시 한번 생각\n",
    "\n",
    "    except UnicodeDecodeError:\n",
    "        with codecs.open(data_file[i], \"r\", \"utf-8\") as f:\n",
    "            texts = f.read()\n",
    "            corpus = k.pos(\"\\n\".join([s for s in texts.split(\"\\n\") if s]))\n",
    "\n",
    "    corpus = append_noun_words(corpus)\n",
    "    corpus_ko = nltk.Text(corpus, name=\"각 판례별 명사 처리\")\n",
    "    corpus_ko_vocab = corpus_ko.vocab()\n",
    "    corpus_ko_vocab_items = corpus_ko_vocab.items()\n",
    "    corpus_ko_vocab_items = list(corpus_ko_vocab_items)  ## TODO: 과연 items순이 옳은가?\n",
    "    #corpus_vocab_common = corpus_ko_vocab.most_common(len(corpus_ko))\n",
    "\n",
    "    ## new_corpus 는 살릴 단어가 하나씩 밖에 안 들어가 있다. 그 개수가 빠져있는 합.\n",
    "    new_corpus = []\n",
    "    corpus_values = []\n",
    "    new_corpus2 = []\n",
    "\n",
    "    for z in range(len(corpus_ko_vocab_items)):\n",
    "        corpus_word = corpus_ko_vocab_items[z][0]\n",
    "        if corpus_word not in word_list:\n",
    "            new_corpus.append([corpus_word])\n",
    "            corpus_values.append(corpus_ko_vocab_items[z][1])\n",
    "    new_corpus2 = sum(new_corpus, [])\n",
    "\n",
    "    last_using_word = []\n",
    "    for j in range(len(new_corpus)):\n",
    "        last_using_word.append(''.join(new_corpus[j]))   # join => 리스트를 문자열로 합치기\n",
    "    \"\"\"\n",
    "    df3 = pd.DataFrame(corpus_values, index=last_using_word)\n",
    "    df3.to_excel(\"C:/Users/user/PycharmProjects/python_study/projectDirectory/POS&embedding/test_dataset/\" \\\n",
    "                 \"TOTAL_GYOTONGSAGO_DATA/TOTAL_GYOTONGSAGO_USING_WORD_LAST({0}).xlsx\".format(i+1),\n",
    "                 sheet_name='단어')\n",
    "    print(\"TOTAL_GYOTONGSAGO_USING_WORD_LAST 저장완료\")\n",
    "    \"\"\"\n",
    "    \n",
    "    ## 이것이 corpus 에서 쓰지 않은 키워드를 갯수만큼 제외시키는 것\n",
    "    word_corpus_last = [x for x in corpus if x not in word_list]\n",
    "    word_corpus_last2 = []\n",
    "    for z in range(len(word_corpus_last)):\n",
    "        word_corpus_last2.append([word_corpus_last[z]])   # word_corpus_last2.append(word_corpus_last[z].split())에서 변경\n",
    "\n",
    "    total_sumWord.append(word_corpus_last2)\n",
    "    # total_sumWord.append(new_corpus)\n",
    "total_sumWord = sum(total_sumWord, [])\n",
    "df4 = pd.DataFrame(total_sumWord)\n",
    "df4.to_excel('C:/Users/Jongil Park/PycharmProjects/ai_study/New_projectDirectory_law2/DataSet/Gyotongsago_data_375/'\n",
    "             'Gyotongsago_resultSet/total_sumWord_for_W2V.xlsx',\n",
    "             sheet_name='최종W2V단어말뭉치')\n",
    "\n",
    "print(\"word2vec 전, 2차 완료(Word2Vec모델에 넣을 단어 말뭉치 형성)\")\n",
    "print(\"len(total_sumWord): \", len(total_sumWord))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec모델에 넣을 최종 63352개의 단어 말뭉치 확보\n",
    "(각 단어의 중복 및 순서 혀용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(w2v_key): 596\n"
     ]
    }
   ],
   "source": [
    "## Word2Vec\n",
    "word2vec_model = Word2Vec(total_sumWord, size=200,     # 인자값에 new_corpus 혹은 word_corpus_last2\n",
    "                          window=2,\n",
    "                          min_count=10,\n",
    "                          workers=4,\n",
    "                          iter=10000, sg=1)\n",
    "\n",
    "w2v_key = word2vec_model.wv.vocab.keys()\n",
    "input_keyword_last = ['테스트를 위한 빈값 넣어놓은 것']\n",
    "print(\"len(w2v_key):\", len(w2v_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec모델을 통해 나온 중복 제외 최종 키워드 갯수: 596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어를 입력하세요(끝내시려면 enter 키를 눌러주세요): 도로에서 음주 후 운전 중에 사고가 났습니다. 혈중알콜농도는 0.15정도 였습니다.\n",
      "len(predict_model):  596\n",
      "(1)차 구동완료\n",
      "검색어를 입력하세요(끝내시려면 enter 키를 눌러주세요): 골목길에서 좌회전 도중 보행자를 차로 치었습니다. 과실 부분은 어떻게 될까요?\n",
      "len(predict_model):  596\n",
      "(2)차 구동완료\n",
      "검색어를 입력하세요(끝내시려면 enter 키를 눌러주세요): 도로에서 직진 중에 좌회전 차량과 접촉 사고가 났습니다.\n",
      "len(predict_model):  596\n",
      "(3)차 구동완료\n",
      "검색어를 입력하세요(끝내시려면 enter 키를 눌러주세요): \n",
      "구동완료\n"
     ]
    }
   ],
   "source": [
    "def input_to_keyword():\n",
    "    input_text = input(\"검색어를 입력하세요(끝내시려면 enter 키를 눌러주세요): \")\n",
    "    k = Komoran()    # 새로 변수를 정의하지 않을시 반복해서 사용자가 검색어를 입력하면 에러가 뜨는 경우가 발생\n",
    "    if input_text is '':\n",
    "        input_keyword_last = None\n",
    "        return input_keyword_last\n",
    "    else:\n",
    "        input_corpus = k.pos(\"\\n\".join([s for s in input_text.split(\"\\n\") if s]))\n",
    "        parsed_input = append_noun_words(input_corpus)\n",
    "        input_keyword_last = []\n",
    "        for j in range(len(parsed_input)):\n",
    "            # if parsed_input[j] not in word_list:\n",
    "            # 위에 문장을 쓰지 않는 이유는 사용자가 검색한 키워드가 항상 우리 키워드 안에 있지는 않지만, 사용자의 키워드를 보여줄 필요는 있다.\n",
    "            input_keyword_last.append(parsed_input[j])\n",
    "        return input_keyword_last\n",
    "\n",
    "repeat_num = 1    # 저장할 액셀 파일에 숫자를 붙여주기 위한 기초값, 첫번째를 뜻함.\n",
    "\n",
    "while True:\n",
    "    input_keyword_last = input_to_keyword()  # 인풋값을 사용할 키워드로 정리한 값\n",
    "    if input_keyword_last is None:\n",
    "        break\n",
    "    columns_keyword = \"·\".join(input_keyword_last)  # 컬럼 이름을 위한 합치기\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    predict_words = []\n",
    "    predict_values = []\n",
    "\n",
    "    ## TODO: 과연 내가 찾으려고 하는 연관성이 predict_output_word()가 맞는가?\n",
    "    predict_model = word2vec_model.predict_output_word(input_keyword_last, topn=len(w2v_key))\n",
    "    for n in range(len(predict_model)):\n",
    "        predict_words.append((predict_model[n][0]))\n",
    "        predict_values.append(predict_model[n][1])\n",
    "    val_words = pd.Series(predict_words)\n",
    "    val_values = pd.Series(predict_values)\n",
    "    df[columns_keyword] = val_words\n",
    "    df['vector값'] = val_values\n",
    "    #word2vec_model.max_final_vocab  # 이것은 왜 필요한거지? 아마 그냥 들어간거 같은데!!??\n",
    "\n",
    "    df.to_csv('C:/Users/Jongil Park/PycharmProjects/ai_study/New_projectDirectory_law2/DataSet/Gyotongsago_data_375/Gyotongsago_resultSet'\n",
    "              '/Gyotongsago_375_W2V_predict({0}).csv'.format(repeat_num),\n",
    "              encoding='CP949')\n",
    "\n",
    "    print(\"len(predict_model): \", len(predict_model))\n",
    "    print(\"({0})차 구동완료\".format(repeat_num))\n",
    "\n",
    "    ## 키워드 연관 단어 상위 20개를 기준으로 한 카운트 값\n",
    "    df_top20 = df[columns_keyword].iloc[:20]   ## iloc와 loc 차이? 여기에선 같은 값이 나오지만 둘의 차이 명확히 알기\n",
    "    count_list_total = []\n",
    "    for kk in range(len(total_panrye_parsingReason)):\n",
    "        count = 0\n",
    "        for j in range(len(df_top20)):\n",
    "            if df_top20[j] in total_panrye_parsingReason[kk]:\n",
    "                # 현재 count 값은 상위 20개를 순서상관없이 + 값, 순서별로 가중치를 주는 방법도 고려할 것.\n",
    "                count += total_panrye_parsingReason[kk][df_top20[j]]\n",
    "        count_list_total.append(count)\n",
    "\n",
    "    # 카운트 값이 높은 순서의 인덱스 값의 리스트를 만들기\n",
    "    df_count = pd.DataFrame()\n",
    "    val_title_list = pd.Series(title_list)\n",
    "    val_keyNum_list = pd.Series(keyNum_list)\n",
    "\n",
    "    df_count = pd.DataFrame({(columns_keyword): count_list_total})\n",
    "    df_count['판례이름'] = val_title_list\n",
    "    df_count['사건번호'] = val_keyNum_list\n",
    "    last_df_count = df_count.sort_values(by=columns_keyword, ascending=False)\n",
    "\n",
    "    last_df_count.to_csv('C:/Users/Jongil Park/PycharmProjects/ai_study/New_projectDirectory_law2/DataSet'\n",
    "                         '/Gyotongsago_data_375/Gyotongsago_resultSet/Gyotongsago_375_W2V_result({0}).csv'.format(repeat_num),\n",
    "                         encoding='CP949')\n",
    "    repeat_num += 1\n",
    "\n",
    "print(\"구동완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>도로·직진·중·좌회전·차량·접촉·사고</th>\n",
       "      <th>vector값</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>피고인</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>반대편</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>취득</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유인</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>차례</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>계약</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>앞쪽</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>청구</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>무시</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>공동정범</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>면허증</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>추락</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>손잡이</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>특별</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>감금</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>방어</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>대리</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>주변</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>집행유예</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>상호</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   도로·직진·중·좌회전·차량·접촉·사고   vector값\n",
       "0                   피고인  0.001678\n",
       "1                   반대편  0.001678\n",
       "2                    취득  0.001678\n",
       "3                    유인  0.001678\n",
       "4                    차례  0.001678\n",
       "5                    계약  0.001678\n",
       "6                    앞쪽  0.001678\n",
       "7                    청구  0.001678\n",
       "8                    무시  0.001678\n",
       "9                  공동정범  0.001678\n",
       "10                  면허증  0.001678\n",
       "11                   추락  0.001678\n",
       "12                  손잡이  0.001678\n",
       "13                   특별  0.001678\n",
       "14                   감금  0.001678\n",
       "15                   방어  0.001678\n",
       "16                   대리  0.001678\n",
       "17                   주변  0.001678\n",
       "18                 집행유예  0.001678\n",
       "19                   상호  0.001678"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어, 사용자의 입력이\n",
    "\"도로에서 직진 중에 좌회전 차량과 접촉 사고가 났습니다.\"일 때,\n",
    "해당 입력값을 파싱하여 Word2Vec모델에 넣고 가장 연관성이 높다고(높은 벡터값) 나온 단어 셋을 만들었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>도로·직진·중·좌회전·차량·접촉·사고</th>\n",
       "      <th>판례이름</th>\n",
       "      <th>사건번호</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>245</td>\n",
       "      <td>폭력행위등 처벌에 관한 법률위반(공동공갈)·업무 방해·공갈·폭력행위등 처벌에 관한 ...</td>\n",
       "      <td>2011노163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>176</td>\n",
       "      <td>살인(예비적죄명:교통사고처리특례법위반)·사기</td>\n",
       "      <td>2015노358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>158</td>\n",
       "      <td>살인(예비적죄명:교통사고처리특례법위반)·사기(남편이 보험금을 노리고 교통사고를 내어...</td>\n",
       "      <td>2017도1549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>133</td>\n",
       "      <td>강도치상·특수강도·도로교통법위반(무면허운전)·사기·교통사고처리특례법위반</td>\n",
       "      <td>2006고합880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>131</td>\n",
       "      <td>폭력행위등 처벌에 관한 법률위반(공동 공갈)·업무 방해·공갈·폭력행위등 처벌에 관한...</td>\n",
       "      <td>2009고합613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>129</td>\n",
       "      <td>감금치사[선택적죄명:살인,인정된죄명:도로교통법위반(사고후미조치)·유기치사]</td>\n",
       "      <td>2013노2492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>115</td>\n",
       "      <td>특정범죄가중처벌등에관한법률위반(도주차량)·교통사고처리특례법위반·도로교통법위반·도로교...</td>\n",
       "      <td>2006노2898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>101</td>\n",
       "      <td>강도치상(인정된죄명:절도·상해)·특수강도·도로교통법위반(무면허운전)·사기·교통사고처...</td>\n",
       "      <td>2007노193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>96</td>\n",
       "      <td>특정범죄가중처벌등에관한법률위반(영리약취·유인등)·아동·청소년의성보호에관한법률위반(강...</td>\n",
       "      <td>2011노573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>87</td>\n",
       "      <td>교통사고처리특례법위반</td>\n",
       "      <td>93노4273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>69</td>\n",
       "      <td>특정범죄가중처벌등에관한법률위반(도주차량, 인정된 죄명 : 교통사고처리특례법위반)</td>\n",
       "      <td>96도591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>51</td>\n",
       "      <td>특정범죄가중처벌등에관한법률위반(도주차량, 인정된 죄명 : 교통사고처리특례법위반)·도...</td>\n",
       "      <td>99도5023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>48</td>\n",
       "      <td>교통사고처리특례법위반</td>\n",
       "      <td>97도1702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>45</td>\n",
       "      <td>교통사고처리특례법위반</td>\n",
       "      <td>98도2605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>44</td>\n",
       "      <td>교통사고처리특례법위반ㆍ업무상과실자동차파괴</td>\n",
       "      <td>83도3006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>44</td>\n",
       "      <td>사기·교통사고처리특례법위반·유가증권위조(변경된죄명:유가증권변조)·위조유가증권행사(변...</td>\n",
       "      <td>2005노396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>44</td>\n",
       "      <td>살인·폭력행위등처벌에관한법률위반·특수공무집행방해치상·공용물건손상·도로교통법위반·향정...</td>\n",
       "      <td>96도2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>42</td>\n",
       "      <td>교통사고처리특례법위반·도로교통법위반(음주운전)</td>\n",
       "      <td>2006노1642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>41</td>\n",
       "      <td>교통사고처리특례법위반·도로교통법위반</td>\n",
       "      <td>96도1540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>39</td>\n",
       "      <td>교통사고처리특례법위반</td>\n",
       "      <td>92도934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     도로·직진·중·좌회전·차량·접촉·사고                                               판례이름  \\\n",
       "354                   245  폭력행위등 처벌에 관한 법률위반(공동공갈)·업무 방해·공갈·폭력행위등 처벌에 관한 ...   \n",
       "36                    176                           살인(예비적죄명:교통사고처리특례법위반)·사기   \n",
       "12                    158  살인(예비적죄명:교통사고처리특례법위반)·사기(남편이 보험금을 노리고 교통사고를 내어...   \n",
       "86                    133            강도치상·특수강도·도로교통법위반(무면허운전)·사기·교통사고처리특례법위반   \n",
       "353                   131  폭력행위등 처벌에 관한 법률위반(공동 공갈)·업무 방해·공갈·폭력행위등 처벌에 관한...   \n",
       "373                   129          감금치사[선택적죄명:살인,인정된죄명:도로교통법위반(사고후미조치)·유기치사]   \n",
       "15                    115  특정범죄가중처벌등에관한법률위반(도주차량)·교통사고처리특례법위반·도로교통법위반·도로교...   \n",
       "85                    101  강도치상(인정된죄명:절도·상해)·특수강도·도로교통법위반(무면허운전)·사기·교통사고처...   \n",
       "340                    96  특정범죄가중처벌등에관한법률위반(영리약취·유인등)·아동·청소년의성보호에관한법률위반(강...   \n",
       "301                    87                                        교통사고처리특례법위반   \n",
       "290                    69       특정범죄가중처벌등에관한법률위반(도주차량, 인정된 죄명 : 교통사고처리특례법위반)   \n",
       "90                     51  특정범죄가중처벌등에관한법률위반(도주차량, 인정된 죄명 : 교통사고처리특례법위반)·도...   \n",
       "287                    48                                        교통사고처리특례법위반   \n",
       "293                    45                                        교통사고처리특례법위반   \n",
       "129                    44                             교통사고처리특례법위반ㆍ업무상과실자동차파괴   \n",
       "76                     44  사기·교통사고처리특례법위반·유가증권위조(변경된죄명:유가증권변조)·위조유가증권행사(변...   \n",
       "281                    44  살인·폭력행위등처벌에관한법률위반·특수공무집행방해치상·공용물건손상·도로교통법위반·향정...   \n",
       "57                     42                          교통사고처리특례법위반·도로교통법위반(음주운전)   \n",
       "286                    41                                교통사고처리특례법위반·도로교통법위반   \n",
       "310                    39                                        교통사고처리특례법위반   \n",
       "\n",
       "          사건번호  \n",
       "354   2011노163  \n",
       "36    2015노358  \n",
       "12   2017도1549  \n",
       "86   2006고합880  \n",
       "353  2009고합613  \n",
       "373  2013노2492  \n",
       "15   2006노2898  \n",
       "85    2007노193  \n",
       "340   2011노573  \n",
       "301    93노4273  \n",
       "290     96도591  \n",
       "90     99도5023  \n",
       "287    97도1702  \n",
       "293    98도2605  \n",
       "129    83도3006  \n",
       "76    2005노396  \n",
       "281    96도2588  \n",
       "57   2006노1642  \n",
       "286    96도1540  \n",
       "310     92도934  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_df_count.iloc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 나온 벡터값이 높은 단어 상위 20개를 활용하여, 각 판례에 20개의 단어 들어있는 갯수대로 카운트를 매겼습니다.\n",
    "위에 가정한대로 카운트 값이 높을수록 해당 판례가 가장 입력값과 연관성이 높은 판례라고 생각했습니다.\n",
    "위에서 확보해놓은 각 판례의 {단어: 갯수}딕셔너리를 통하여 카운트 값이 가장 큰 값 순으로 정렬하였습니다.\n",
    "\n",
    "DataFrame의 내용은\n",
    "맨 왼쪽의 숫자는 1-375개의 판례의 순서 번호이며,\n",
    "중간 숫자는 해당 키워드('도로·직진·중·좌회전·차량·접촉·사고')의 카운트값\n",
    "카운트값이 높은 순서대로 판례이름ㅁ과 해당 사건번호 순서입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_kernel",
   "language": "python",
   "name": "ai_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
